{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI using watsonx example\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "To run as python code you will need to create and configure a virtual environment with python 3.12. There are many guides online, but typically it looks similar to the following \n",
    "```\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate \n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**OR ALTERNATIVELY IF YOU USE VSCODE**\n",
    "\n",
    "```\n",
    "Ctrl + Shift + P\n",
    "Type and Select \"Python: Create Environment...\"\n",
    "Select Venv\n",
    "Select Python 3.12 Kernel\n",
    "Activate venv through opening a new terminal or type \".venv\\Scripts\\activate\" in terminal\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables from __.env__ file\n",
    "\n",
    "**IMPORTANT: DO NOT SHARE __.env__ ANYWHERE AND DO NOT UPLOAD IT TO GIT. KEEP IT ON YOUR LOCAL MACHINE ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_id = os.getenv('PROJECT_ID')\n",
    "api_key=os.getenv('GENAI_API_KEY')\n",
    "url=os.getenv('GENAI_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify your prompt and parameters below. \n",
    "\n",
    "You can find more documentation online should you need it. Each model will have a unique prompt template - many of these are found online. For example, multi-turn conversation prompts for llama3 can be found [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "\n",
    "**IMPORTANT: KEEP YOUR PROMPTS APPROPRIATE AS THE API CALLS ARE MONITORED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your prompt below. \n",
    "prompt = '''\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Why is the sky blue?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Tokens can be thought of as part words - almost like syllables. \n",
    "# Larger MAX_NEW_TOKENS will allow the models to generate longer responses.\n",
    "# Each model has a unique context window for tokens - for llama3 it is 8000 tokens. \n",
    "generate_params = {\n",
    "            GenParams.MAX_NEW_TOKENS: 300\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with different models by changing the `model_id` parameter below; Some models to try out might be: \n",
    "\n",
    "- ibm/granite-34b-code-instruct\n",
    "- meta-llama/llama-3-70b-instruct\n",
    "- ibm/granite-8b-code-instruct\n",
    "- meta-llama/llama-3-8b-instruct\n",
    "- ibm/granite-13b-chat-v2\n",
    "\n",
    "Remember that each model will have its own unique prompt template, and strengths and weaknesses. For example;\n",
    "- instruction tuned models will expect an instruction, \n",
    "- chat tuned models will give answers closer to natural language chats, \n",
    "- code models will excel at writing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it is made up of different wavelengths, including red, orange, yellow, green, blue, and violet. Shorter-wavelength light, such as blue and violet, is scattered in all directions more than longer-wavelength light. As a result, the scattered blue and violet light reach our eyes from all directions, making the sky appear blue. This scattering also causes the sun to appear yellow during sunrise and sunset, as the shorter-wavelength light is scattered away from our line of sight, leaving the longer-wavelength light, like red and orange, to dominate.\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    model_id=\"ibm/granite-13b-chat-v2\",\n",
    "    params=generate_params,\n",
    "    credentials={\n",
    "        \"apikey\": f\"{api_key}\",\n",
    "        \"url\": f\"{url}\"\n",
    "    },\n",
    "    project_id=project_id\n",
    "    )\n",
    "\n",
    "generated_response = model.generate(prompt=prompt)\n",
    "print(generated_response['results'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Model(\n",
    "    model_id=\"ibm/granite-13b-chat-v2\",\n",
    "    params=generate_params,\n",
    "    credentials={\n",
    "        \"apikey\": f\"{api_key}\",\n",
    "        \"url\": f\"{url}\"\n",
    "    },\n",
    "    project_id=project_id\n",
    "    )\n",
    "\n",
    "generated_response = model.generate(prompt=prompt)\n",
    "print(generated_response['results'][0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
